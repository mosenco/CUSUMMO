{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"flowchart\" per l'applicazione.\n",
    "\n",
    "1. lettura e preprocessing dei dati\n",
    "2. lavorazione sul modello\n",
    "3. generazione output \n",
    "4. valutazione output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tikyn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\aeon\\base\\__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from claspy.segmentation import BinaryClaSPSegmentation\n",
    "\n",
    "import ruptures as rpt\n",
    "\n",
    "import stumpy\n",
    "from aeon.segmentation import find_dominant_window_sizes\n",
    "\n",
    "from aeon.segmentation import GreedyGaussianSegmenter\n",
    "\n",
    "from aeon.segmentation import InformationGainSegmenter\n",
    "\n",
    "from aeon.anomaly_detection import STRAY\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer,mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from aeon.visualisation import plot_series_with_change_points, plot_series_with_profiles\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_color_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questi sono tutte le features estratte\n",
    "# kineticglobal,kineticchest,directnesshead,density,leftwristke,rightwristke,leftankleke,rightankleke,headke,posturaltension\n",
    "\n",
    "# Queste sono le feature che utilizzeremo\n",
    "# kineticglobal, density, leftwirstke, rightwristke, leftankleke, rightankleke, headke, posturaltension\n",
    "# che corrispondono\n",
    "# 2,4,5,6,7,8,9,10,11\n",
    "# Questa funzione legge il file di input e restituisce un dataframe con i dati preprocessati\n",
    "def ReadAndPreProcess(inputDataRaw):\n",
    "    # lettura\n",
    "    df=pd.read_csv(inputDataRaw,sep=' ', header=None)\n",
    "    df.drop(0, axis=1)\n",
    "    df[\"som\"]=df.sum(axis=1)\n",
    "    ## preprocessing\n",
    "\n",
    "    # dealing NaN values\n",
    "    #-serafino ha usato forward fill, backward fill, linear interpolation\n",
    "    #-ricordo che serafino aveva gia utilizzato sta cosa sui dati grezzi non sulle feature ma sui dati prefeature percio dovrebbe essere gia apposto\n",
    "\n",
    "    # downsampling\n",
    "    #-sono a 100ms, non sò se devo scendere a 50ms. da decidere\n",
    "\n",
    "    # low pass filter\n",
    "    #-Skogstad and colleagues (2013) e https://stackoverflow.com/questions/25191620/creating-lowpass-filter-in-scipy-understanding-methods-and-units\n",
    "    #-implementare dopo\n",
    "\n",
    "    # remove outliers\n",
    "    #-utilizzare hampel filter\n",
    "\n",
    "    # stretch\n",
    "    #-forse devo stretcharlo come ho fatto precedentemente\n",
    "    #-anche se nel codice precedente ho stretchato solo il groundtruth\n",
    "\n",
    "    # ritorno un oggetto dataframe dopo che è stato lavorato, ottenendo un prodotto lavorato\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questa funzione ritorna un dataframe del groundtruth che viene usato specificatamente per visualizzare il gt\n",
    "# è soggetto a un preprocessing dei dati siccome l'ultimo groundtruth è dove termina il ts del gt\n",
    "# di conseguenza per farlo corrispondere, bisogna stretcharlo\n",
    "# ma ricordo di aver rifatti i dati nuovi per generare un groundtruth a fine ts, da controllare cosi che non serve stretcharlo?\n",
    "def LoadingGroundTruth(df,gtraw):\n",
    "    gt=pd.read_csv(gtraw,sep=' ', header=None)\n",
    "    gt=gt.iloc[:,0].values\n",
    "    #stretching dei dati se necessario per farlo corrispondere alla ts dei dati\n",
    "    stretch_gt = np.array([])\n",
    "    for idx,i in enumerate(gt):\n",
    "        relpos = len(df)*i/gt[-1]\n",
    "        stretch_gt = np.append(stretch_gt,relpos)\n",
    "        \n",
    "    return stretch_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizzo CLASP \n",
    "# prende come parametro un dataframe e restituisce il clasp score\n",
    "# gt e known vengono usati per usare il numero vero di cp se uguale a 1 sennò si cerca di predirlo se il modello lo permette\n",
    "def GetClasp(df,gt,known):\n",
    "    #result=np.array([])\n",
    "    result=np.array([])\n",
    "    eachresult = []\n",
    "    for idx,i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "      \n",
    "        ts=df.iloc[:,i]\n",
    "\n",
    "        if known == 1:\n",
    "            clasp = BinaryClaSPSegmentation(n_segments=len(gt))\n",
    "        else:\n",
    "            clasp = BinaryClaSPSegmentation()\n",
    "            \n",
    "        found_cps = clasp.fit_predict(ts.values)    \n",
    "\n",
    "        # per ogni array di cp di ogni singola feature\n",
    "        # li unisco in un unico array. in pratica faccio un OR di tutti i cp\n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "        #result = np.sort(np.append(result,current_result).flatten())\n",
    "\n",
    "        \n",
    "    return result, eachresult, clasp\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizzo BINSEG\n",
    "def GetBinseg(df,gt,known):\n",
    "    result=np.array([])\n",
    "    eachresult=[]\n",
    "    for idx, i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "        \n",
    "        ts=df.iloc[:,i]\n",
    "        binseg = rpt.Binseg(model=\"ar\").fit(ts)\n",
    "\n",
    "        if known == 1:\n",
    "            found_cps = np.array(binseg.predict(n_bkps=len(gt)), dtype=np.int64)\n",
    "        else:\n",
    "            found_cps = np.array(binseg.predict(pen=np.log(len(ts))), dtype=np.int64)\n",
    "        \n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "\n",
    "    return result, eachresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizzo WINDOW\n",
    "def GetWindowSlide(df,gt,known):\n",
    "    result=np.array([])\n",
    "    eachresult=[]\n",
    "    for idx, i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "        \n",
    "        ts=df.iloc[:,i]\n",
    "        binseg = rpt.Window(model=\"ar\").fit(ts)\n",
    "\n",
    "        if known==1:\n",
    "            found_cps = np.array(binseg.predict(n_bkps=len(gt)), dtype=np.int64)\n",
    "        else:\n",
    "            found_cps = np.array(binseg.predict(pen=np.log(len(ts))), dtype=np.int64)\n",
    "        \n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "\n",
    "    return result, eachresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizzo FLUSS\n",
    "def GetFluss(df,gt,known):\n",
    "    result=np.array([])\n",
    "    eachreuslt=[]\n",
    "    for idx, i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "\n",
    "        ts=df.iloc[:,i]\n",
    "        window_size = min(find_dominant_window_sizes(ts) * 2, len(ts) // 2)\n",
    "        mp = stumpy.stump(ts, m=window_size)\n",
    "\n",
    "        if known == 1:\n",
    "            cac, found_cps = stumpy.fluss(mp[:, 1], L=window_size, n_regimes=len(gt))\n",
    "        else:\n",
    "            cac, found_cps = stumpy.fluss(mp[:, 1], L=window_size, n_regimes=4)\n",
    "        \n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "\n",
    "    return result, eachresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizzo GSS\n",
    "def GetGSS(df,gt,known):\n",
    "    result=np.array([])\n",
    "    result=[]\n",
    "    for idx,i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "\n",
    "        ts=df.iloc[:,i]\n",
    "        \n",
    "        if known==1:\n",
    "            ggs = GreedyGaussianSegmenter(k_max=len(gt))\n",
    "        else:\n",
    "            ggs = GreedyGaussianSegmenter(k_max=1)\n",
    "\n",
    "\n",
    "        ggs.fit_predict(ts)\n",
    "        found_cps = ggs.change_points_\n",
    "\n",
    "        if (len(found_cps) >= 2):\n",
    "            # remove start and end of the series\n",
    "            found_cps = found_cps[1:-1] \n",
    "\n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult=[]\n",
    "        \n",
    "    return result,eachresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizzo IGS\n",
    "def GetIGS(df,gt,known):\n",
    "    result=np.array([])\n",
    "    eachresult=[]\n",
    "    for idx,i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "\n",
    "        ts=df.iloc[:,i]\n",
    "\n",
    "        if known==1:\n",
    "            igts = InformationGainSegmenter(k_max=len(gt))\n",
    "        else:\n",
    "            igts = InformationGainSegmenter(k_max=1)\n",
    "\n",
    "        igts.fit_predict(ts)\n",
    "        found_cps = igts.change_points_\n",
    "\n",
    "        if (len(found_cps) >= 2):\n",
    "            # remove start and end of the series\n",
    "            found_cps = found_cps[1:-1] \n",
    "\n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "\n",
    "    return result, eachresult\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utiizzo STRAY\n",
    "def GetSTRAY(df,gt,known):\n",
    "    result=np.array([])\n",
    "    eachresult=[]\n",
    "    for idx,i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "\n",
    "        ts=df.iloc[:,i]\n",
    "        stray = STRAY()\n",
    "        stray.fit(ts)\n",
    "\n",
    "        if known==1:\n",
    "            found_cps = np.sort(np.argpartition(-stray.score_, len(gt))[:len(gt)])\n",
    "        else:\n",
    "            found_cps = np.sort(np.argpartition(-stray.score_, 1)[:1])\n",
    "\n",
    "        if (len(found_cps) >= 2):\n",
    "            # remove start and end of the series\n",
    "            found_cps = found_cps[1:-1] \n",
    "\n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "\n",
    "    return result, eachresult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fakeplot(df,gt,cp):\n",
    "    _ = plot_series_with_change_points(df.iloc[:,7], cp[4], title=\"prediction number: \"+str(7))\n",
    "\n",
    "    plt.figure(figsize=(18,9))\n",
    "    plt.plot(np.arange(len(df[7].values)),df[7].values,'blue',linewidth=0.5)\n",
    "    for idx2,j in enumerate(gt.astype(int)):\n",
    "\n",
    "        plt.axvline(x = j, color = 'green',linewidth=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotResult(df,gt,cp,eachcp):\n",
    "    #da testare quando ho piu valori\n",
    "    #clasp.plot(gt_cps=gt.astype(int), heading=\"Segmentation of different umpire cricket signals\", ts_name=\"ACC\", file_path=\"segmentation_example.png\")\n",
    "\n",
    "        \n",
    "    for idx,i in enumerate([2,4,5,6,7,8,9,10,11]):\n",
    "  \n",
    "        # idx == 8 è l'ultimo elemento. percio quando i è uguale a 11\n",
    "        # significa che l'ultimo elemento del mio for è la ts totale di tutte le feature\n",
    "        if idx == 8:\n",
    "            plt.figure(figsize=(18,9))\n",
    "            plt.plot(np.arange(len(df[\"som\"].values)),df[\"som\"].values,'blue',linewidth=0.5)\n",
    "            for idx2,j in enumerate(gt.astype(int)):\n",
    "\n",
    "                plt.axvline(x = j, color = 'green',linewidth=1) \n",
    "\n",
    "            for j in cp.tolist():\n",
    "                plt.axvline(x = j, color = 'black',linewidth=1) \n",
    "        else:\n",
    "\n",
    "\n",
    "            \n",
    "            _ = plot_series_with_change_points(df.iloc[:,i], eachcp[idx], title=\"prediction number: \"+str(i))\n",
    "            for idx2,j in enumerate(gt.astype(int)):\n",
    "                plt.axvline(x = j, color = 'green',linewidth=1) \n",
    "\n",
    "        \"\"\"ArithmeticError\n",
    "        _ = plot_series_with_profiles(\n",
    "                df.iloc[:,i],\n",
    "                clasp.profile,\n",
    "                true_cps=gt,\n",
    "                found_cps=cp[idx],\n",
    "                title=\"Electric Devices\",\n",
    "            )\"\"\"\n",
    "\n",
    "\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcola i vari scores dati il groundtruth e il prediction\n",
    "# puo salvare il risultato su file per evitare di perderli\n",
    "# prende come parametro nome del groundtruth, groundtruth, nome della timeseries e il prediction\n",
    "def Evaluate(modelName,gtName, gt, tsName, cp, df):\n",
    "    # creo dei array di lunghezza come la ts cosi possono fare il confronto\n",
    "    # sia per il gt che per il pd\n",
    "  \n",
    "    cpnump = np.array(cp)\n",
    "    gtnump = np.array(gt)\n",
    "\n",
    "    cp_long = np.zeros(len(df)+1)\n",
    "    cp_long[cpnump.astype(int)]=1\n",
    "\n",
    "    gt_long = np.zeros(len(df)+1)\n",
    "    gt_long[gtnump.astype(int)]=1\n",
    "\n",
    "    accuracy = accuracy_score(gt_long, cp_long)\n",
    "    precision = precision_score(gt_long,cp_long)\n",
    "    recall = recall_score(gt_long,cp_long)\n",
    "    f1 = f1_score(gt_long,cp_long)\n",
    "    print(f'accuracy, precision, recall, f1: {accuracy,precision,recall,f1}')\n",
    "    \n",
    "    #scrivo su file il risultato\n",
    "    f = open(str(tsName), \"a\")\n",
    "    f.write(\"model:\"+str(modelName)+\"\\n\")\n",
    "    f.write(\"Timeseries:\"+str(tsName)+\" accuracy:\"+str(accuracy)+\" precision:\"+str(precision)+\" recall:\"+str(recall)+\" f1:\"+str(f1)+\" \\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questi sono i dataset comprendente le timeseries e il groundtruth\n",
    "timeseries=[#\"in\\cora1_input.txt\",\n",
    "      #\"in\\cora4_input.txt\",\n",
    "    #\"in\\cora1_input.txt\",\n",
    "    #  \"in\\cora4_input.txt\",\n",
    "      \"in\\cora5_input.txt\",\n",
    "      #\"in\\cora14_input.txt\",\n",
    "     # \"in\\cora14_input.txt\",\n",
    "      \"in\\marianne7_input.txt\",\n",
    "      \"in\\marianne8_input.txt\",\n",
    "      \"in\\marianne10_input.txt\",\n",
    "      \"in\\marianne18_input.txt\",\n",
    "      \"in\\marianne19_input.txt\",\n",
    "      \"in\\marianne24_input.txt\",\n",
    "      \"in\\marianne26_input.txt\",\n",
    "      \"in\\marianne41_input.txt\",\n",
    "      \"in\\marianne42_input.txt\",\n",
    "      \"in\\marianne43_input.txt\",\n",
    "      \"in\\marianne47_input.txt\",\n",
    "      #\"in\\marianne48_input.txt\",\n",
    "   #   \"in\\marianne48_input.txt\",\n",
    "      \"in\\muriel18_input.txt\",\n",
    "      \"in\\muriel26_input.txt\",\n",
    "      \"in\\muriel27_input.txt\",\n",
    "      #\"in\\muriel30_input.txt\"\n",
    "     # \"in\\muriel30_input.txt\"\n",
    "      ]\n",
    "groundtruth=[#\"gt\\cora_gt_2019-08-08_t001_video01.txt\",\n",
    "         #\"gt\\cora_gt_2019-08-08_t004_video01.txt\",\n",
    "          #  \"gt\\cora_gt_2019-08-08_t001_video01.txt\",\n",
    "         #\"gt\\cora_gt_2019-08-08_t004_video01.txt\",\n",
    "         \"gt\\cora5_gt.txt\",\n",
    "         \"gt\\cora_gt_2019-08-08_t014_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t007_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t008_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t010_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t018_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t019_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t024_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t026_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t041_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t042_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t043_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t047_video01.txt\",\n",
    "         #\"gt\\marianne_gt_2016-03-22_t048_video01.txt\",\n",
    "       #  \"gt\\marianne_gt_2016-03-22_t048_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-21_t014_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-21_t018_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-21_t026_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-21_t027_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-23_t029_video01.txt\",\n",
    "         #\"gt\\muriel_gt_2016-03-23_t030_video01.txt\",\n",
    "        # \"gt\\muriel_gt_2016-03-23_t030_video01.txt\"\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASP with known number of cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp,clasp=GetClasp(df,gt,1)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"CLASPk\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASP without knowing cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp,clasp=GetClasp(df,gt,0)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"CLASP\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINSEG with known number of cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetBinseg(df,gt,1)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"BINSEGw\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINSEG without knowing cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetBinseg(df,gt,0)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"BINSEG\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWSLIDE with known number of cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetWindowSlide(df,gt,1)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"WINDOWw\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWSLIDE without knowing cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetWindowSlide(df,gt,0)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"WINDOW\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUSS with known number of cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetFluss(df,gt,1)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"FLUSSw\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUSS without knowing cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetFluss(df,gt,0)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"FLUSS\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSS with known number of cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetGSS(df,gt,1)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"GSSw\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSS without knowing cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetGSS(df,gt,0)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"GSS\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGS with known number of cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetIGS(df,gt,1)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"IGSw\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGS without knowing cp\n",
    "for i in range(0,len(timeseries)):\n",
    "    df=ReadAndPreProcess(timeseries[i])\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp,eachcp=GetIGS(df,gt,0)\n",
    "    PlotResult(df,gt,cp,eachcp)\n",
    "    Evaluate(\"IGS\",groundtruth[i],gt,timeseries[i],cp,df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
