{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tikyn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\aeon\\base\\__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from core import *\n",
    "from modelparamgrid import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The whole model works in this order</b>\n",
    "<ol>\n",
    "    <li>clasp model - in: features ts [21][98][n] - out: prediction [21][98][m] - n>m</li>\n",
    "    <ul>\n",
    "        <li>21 video, ogni video 98 timeseries</li>\n",
    "        <li>per ogni 98 timeseries, 98 prediction</li>\n",
    "        <li>n = lunghezza timeseries</li>\n",
    "        <li>m = numeri dei prediction</li>\n",
    "    </ul>\n",
    "    <li>split saliency - in: prediction [21][98][m] - out: pred_split [21][6][k][m]</li>\n",
    "    <ul>\n",
    "        <li>98 prediction divisi in 6 gruppi di k elementi. Possono esserci ripetizioni. 6*k > 98</li>\n",
    "        <li>k = numero di prediction in un gruppo\n",
    "    </ul>\n",
    "    <li>combinesinglesaliency - in: pred_split[21][6][k][m] - out: salience_preds [21][6][x] - k>x</li>\n",
    "    <ul>\n",
    "        <li>ogni k prediction unite in 1 array e applicato MajorityVote</li>\n",
    "        <li>x = MajorityVote genera la media di gruppi di prediction k</li>\n",
    "    </ul>\n",
    "    <li>union salience_preds - in: salience_preds [21][6][x] - out: union_pred [21][y] - y>x</li>\n",
    "    <ul>\n",
    "        <li>Le 6 salienze unite in un unico array</li>\n",
    "        <li>y = unione di x\n",
    "    </ul>\n",
    "    <li>ignorezone - in: union_pred [21][y] - out: ignored_pred [21][a] - y>a</li>\n",
    "    <ul>\n",
    "        <li>Ignora le zone non annotate perch√® fuori dal nostro use case</li>\n",
    "        <li>a = numero di prediction dopo aver tolto i prediction dalle zone non annotate</li>\n",
    "    </ul>\n",
    "    <li>delnear - in: ignored_pred [21][a] - out: del_pred [21][b]</li>\n",
    "    <ul>\n",
    "        <li>Come MajorityVote ma lascia primo e ultimo del gruppo</li>\n",
    "        <li>b = numero di prediction che delineano inizio e fine di movimenti</li>\n",
    "    </ul>\n",
    "    <li>DTW - in: del_pred [21][b] - out: dtw_pred [21][c] - b>c</li>\n",
    "    <ul>\n",
    "        <li>applico DTW per controllare se i prediction b hanno FP (false positive)</li>\n",
    "        <li>c = numero di prediction con meno FP</li>\n",
    "    </ul>\n",
    "    <li>scoring - in: dtw_pred [21][c] - out: f1score [21]</li>\n",
    "    <ul>\n",
    "        <li>Per ogni prediction lungo c nei 21 video, calcolo lo score</li>\n",
    "    </ul>\n",
    "</ol>\n",
    "\n",
    "This project is divided into three part, because each part takes a lot of days for computing the result:\n",
    "<ol>\n",
    "    <li>Clasp Model: (1) -> create all prediction from the clasp model</li>\n",
    "    <li>Filtering: (2)(3)(4)(5)(6)(7) -> filters all prediction to remove false positives</li>\n",
    "    <li>Scoring: (8) -> given the filtered prediction, gives an evaluation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To find the perfect Hyperparameters we use gridsearch in this order</b>\n",
    "<ol>\n",
    "    <li>Clasp Model - in: [21][98][n] - out: [20][21][98][n]</li>\n",
    "    <ul>\n",
    "        <li>Gridsearch just for model generated 20 different clasp models</li>\n",
    "    </ul>\n",
    "    <li>Filtering - in: [20][21][98][n] - out: [20][243][21][c]</li>\n",
    "    <ul>\n",
    "        <li>Gridsearch for filtering generated 243 different results</li>\n",
    "    </ul>\n",
    "    <li>Scoring LOVO - in:[20][243][21][c] - out: f1score + hyperparameters</li>\n",
    "    <ul>\n",
    "        <li>Try everything and return an f1score and which hyperparameters are the best</li>\n",
    "    </ul>\n",
    "    \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Clasp Model (1)</b>\n",
    "We use ParameterGrid() to compute all the combination given the param_grid and we obtained <b>20</b> different models\n",
    "<p>in: features ts [21][98][n] - out: prediction [20][21][98][m]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all selected hyperparameters values to gridsearch\n",
    "param_grid = {\n",
    "    \"window_size\": ['suss'],\n",
    "    \"distance\": ['euclidean_distance'],\n",
    "    \"score\":['roc_auc','f1'],\n",
    "    \"early_stopping\": [True,False],\n",
    "    \"k_neighbours\":[1,3,5,7,9],\n",
    "    \"n_jobs\":[4]\n",
    "}\n",
    "\n",
    "\n",
    "# len(ParameterGrid(param_grid)) = 20\n",
    "for p_idx,params in enumerate(ParameterGrid(param_grid)):\n",
    "    #excl_radius has to be larger than neighbours\n",
    "    params[\"excl_radius\"]=params[\"k_neighbours\"]+2\n",
    "    # cp = prediction of 1 timeseries \n",
    "    # cps = prediction for n timeseries\n",
    "    # cpsl = prediction for n timeseries for m different hyperparameters \n",
    "    cpsl=[]\n",
    "    for i in range(len(TIMESERIES)):\n",
    "        df=ReadAndPreProcess(TIMESERIES[i])\n",
    "        cp_all,cps,clasp=GetClasp2(df,list(range(len(FEATURES_NAME))),p_idx,i, **params)\n",
    "        cpsl.append(cps)\n",
    "\n",
    "    # COMMENTED TO AVOID OVERWRITE ALREADY COMPUTED FILE\n",
    "    # save prediction of [20][21][98] = [param][video][feature]\n",
    "    #with open(\"../cpsl_param_\"+str(p_idx)+\".pkl\", \"wb\") as f:\n",
    "        #pickle.dump(cpsl, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Filtering (2)(3)(4)(5)(6)(7)</b>\n",
    "The gridsearch isn't finished yet. Each step has its known set of parameters that needs to be found to have the optimal model. So in the filtering phase we gridsearch all values and generate all possible filtered prediction from each different. \n",
    "\n",
    "The computational costs is made by for loops:\n",
    "<p>for each models = 19</p>\n",
    "<p style=\"text-indent: 15px;\">for each params = 243 </p>\n",
    "<p style=\"text-indent: 30px;\">for each videos = 21 </p>\n",
    "<p style=\"text-indent: 45px;\">for each features = 98</p>\n",
    "<p>in:[20][21][98] - out:[20][21][c]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe for 21 videos\n",
    "with open(\"../dfl.pkl\", \"rb\") as f:\n",
    "    dfl = pickle.load(f)\n",
    "\n",
    "#load groundtruth for 21 videos\n",
    "with open(\"../gtl.pkl\", \"rb\") as f:\n",
    "    gtl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous step, load 20 models in modelpred\n",
    "modelpred=[]\n",
    "for i in range(0,20):\n",
    "    with open(\"../res/cpsl_param_\"+str(i)+\".pkl\", \"rb\") as f:\n",
    "        modelpred.append(pickle.load(f))\n",
    "\n",
    "#20 model\n",
    "#21 video\n",
    "#98 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. from 98 predictions, divide it into 6 groups\n",
    "# Some prediction could be repeated in the groups\n",
    "def SplitSaliency(pred):\n",
    "    # 2. split features in 6 saliency groups\n",
    "    # vel,acc,jerk,specific,generic\n",
    "    # specific: other type of feature specific for that saliency\n",
    "    # generic: other type of feature that describe the whole body movement\n",
    "    ## generic features: 0,3\n",
    "    ariel_el=[26,27,28,29,30,31,32,33,34,35,36,37,2,8,14,0,3]\n",
    "    ariel = UnionCPS(*pred[ariel_el])\n",
    "   \n",
    "    strn_el=[38,39,40,41,42,43,44,45,46,47,48,49,1,9,0,3]\n",
    "    strn = UnionCPS(*pred[strn_el])\n",
    "\n",
    "    rhel_el=[50,51,52,53,54,55,56,57,58,59,60,61,7,11,16,17,18,21,0,3]\n",
    "    rhel = UnionCPS(*pred[rhel_el])\n",
    "\n",
    "    lhel_el=[62,63,64,65,66,67,68,69,70,71,72,73,6,10,16,17,19,20,0,3]\n",
    "    lhel = UnionCPS(*pred[lhel_el])\n",
    "    \n",
    "    rplm_el=[74,75,76,77,78,79,80,81,82,83,84,85,5,13,15,23,25,0,3]\n",
    "    rplm = UnionCPS(*pred[rplm_el])\n",
    "\n",
    "    lplm_el=[86,87,88,89,90,91,92,93,94,95,96,97,4,12,15,22,24,0,3]\n",
    "    lplm = UnionCPS(*pred[lplm_el])\n",
    "\n",
    "    return [ariel,strn,rhel,lhel,rplm,lplm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. given one list, determine the mean value in the group if it's within maj_margin\n",
    "#    and decide if to compute the mean if the number of el in the group is over maj_thre\n",
    "def CombineSingleSaliency(saliencies, maj_margin, maj_thre):\n",
    "    res=[]\n",
    "    for sal in saliencies:\n",
    "        res.append(MajorityVoteCP(sal, maj_margin, maj_thre))\n",
    "    return res\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the filtering part for just one prediction.\n",
    "# after we computed all prediction from 20 different models\n",
    "# all prediction should be filtered to be combined and filtered to remove False Positive\n",
    "# in: prediction\n",
    "def PostFilterTogether(model_id,video_id,param_id,param_len,density,pred,gt, maj_margin, maj_thre, zone_margin, del_margin, dtw_threshold):\n",
    "    \"\"\"\n",
    "        pred: 98 predictions of a single video\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"model:{model_id}/20 - video:{video_id}/21 - param:{param_id}/{param_len} - Start                                               \", end='\\r')\n",
    "\n",
    "    #2. from a list[98] divide it into 6 lists. Some prediction could be repeated into diff groups\n",
    "    saliencies_raw = SplitSaliency(pred)\n",
    "    print(f\"model:{model_id}/20 - video:{video_id}/21 - param:{param_id}/{param_len} - Split Saliency completed                            \", end='\\r')\n",
    "    #3. each group has different list of prediction. combine it into 1 and apply majorityvote\n",
    "    saliencies = CombineSingleSaliency(saliencies_raw,maj_margin,maj_thre)\n",
    "    print(f\"model:{model_id}/20 - video:{video_id}/21 - param:{param_id}/{param_len} - Combine Saliency completed                          \", end='\\r')\n",
    "    #4. from 6 list, combine into 1 list\n",
    "    pred_united = UnionCPS(*saliencies)\n",
    "    print(f\"model:{model_id}/20 - video:{video_id}/21 - param:{param_id}/{param_len} - Union completed                                     \", end='\\r')\n",
    "    #5. there is no annotated zone. remove prediction from that subset\n",
    "    pred_ignored = IgnoreZone(video_id,pred_united,gt,zone_margin)\n",
    "    print(f\"model:{model_id}/20 - video:{video_id}/21 - param:{param_id}/{param_len} - Ignore Zone completed                               \", end='\\r')\n",
    "    #6. apply motion fusion: in a group within del_margin, remove everything except 1 and last\n",
    "    pred_del = delnear(pred_ignored,del_margin)\n",
    "    print(f\"model:{model_id}/20 - video:{video_id}/21 - param:{param_id}/{param_len} - delnear completed                                   \", end='\\r')\n",
    "    #7. check the results with DTW to remove potential FP\n",
    "    pred_final=FPremoverDTW(density,pred_del,dtw_threshold)\n",
    "    print(f\"model:{model_id}/20 - param:{param_id}/{param_len} - video:{video_id}/21 - DTW completed                                       \", end='\\r')\n",
    "    # return a list of prediction\n",
    "    return pred_final\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering part for all prediction from all 20 models\n",
    "#gridsearch params for our filtering part\n",
    "param_grid = {\n",
    "    \"maj_margin\": [80,100,120],\n",
    "    \"maj_thre\": [1,2,3],\n",
    "    \"zone_margin\":[80,100,120],\n",
    "    \"del_margin\": [80,100,120],\n",
    "    \"dtw_threshold\":[80,100,120]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#for each model, for each params,\n",
    "# compute for each video the filtering\n",
    "# each leaf would be a prediction with a specific model and specific filter_param\n",
    "model_res=[]\n",
    "for model_id,model in enumerate(modelpred): #20 models predictions\n",
    "    param_value=[]\n",
    "    for p_idx,params in enumerate(ParameterGrid(param_grid)): #243\n",
    "        video=[]\n",
    "        for i in range(len(TIMESERIES)): #21 videos\n",
    "            df = dfl[i]\n",
    "            gt = gtl[i]\n",
    "            res=PostFilterTogether(model_id,i,p_idx,len(ParameterGrid(param_grid)),df.iloc[:,3],np.array(model[i],dtype=\"object\"),gt,params[\"maj_margin\"],params[\"maj_thre\"],params[\"zone_margin\"],params[\"del_margin\"],params[\"dtw_threshold\"])\n",
    "            video.append(res)\n",
    "       \n",
    "        param_value.append(video)\n",
    "    model_res.append(param_value)\n",
    "\n",
    "# COMMENTED TO AVOID OVERWRITE LOCAL FILES\n",
    "# save the filtered prediction in local\n",
    "#with open(\"../collection_models_prediction_filtered.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(model_res, f)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scoring (8)</b>\n",
    "here we take as input all the filtered results and performs a cross-validation with our predictions to see which hyperparameters for our model+filter is the best and then test it on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe for 21 videos\n",
    "with open(\"../dfl.pkl\", \"rb\") as f:\n",
    "    dfl = pickle.load(f)\n",
    "\n",
    "#load groundtruth for 21 videos\n",
    "with open(\"../gtl.pkl\", \"rb\") as f:\n",
    "    gtl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the filtered prediction\n",
    "with open(\"../collection_models_prediction_filtered.pkl\", \"rb\") as f:\n",
    "    collection_models_prediction_filtered = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=collection_models_prediction_filtered\n",
    "# model:20 params:243 video:21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the f1 margin\n",
    "# se il suss mi individua quella sezione dove ha propriet√† statiche\n",
    "# allora il prossimo changepoint non pu√≤ essere minore di quello?\n",
    "res=[]\n",
    "for i in range(len(TIMESERIES)):\n",
    "    df=dfl[i]\n",
    "    for j in range(0,98):\n",
    "        res.append(suss(df.iloc[:,j].values))\n",
    "f1_margin=statistics.mean(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_margin=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\\cora1_in.txt 0.6176470588235293\n",
      "in\\cora4_05_in.txt 0.7631578947368421\n",
      "in\\cora4_08_in.txt 0.9333333333333333\n",
      "in\\cora5_in.txt 0.5777777777777777\n",
      "in\\cora14_in.txt 0.6\n",
      "in\\marianne7_in.txt 0.75\n",
      "in\\marianne8_in.txt 0.782608695652174\n",
      "in\\marianne10_in.txt 0.8260869565217392\n",
      "in\\marianne18_in.txt 0.7857142857142856\n",
      "in\\marianne19_in.txt 0.6666666666666667\n",
      "in\\marianne24_in.txt 0.6363636363636364\n",
      "in\\marianne26_in.txt 0.7058823529411765\n",
      "in\\marianne41_in.txt 0.5714285714285715\n",
      "in\\marianne42_in.txt 0.6363636363636364\n",
      "in\\marianne43_in.txt 0.5977011494252873\n",
      "in\\marianne47_in.txt 0.7586206896551724\n",
      "in\\marianne48_in.txt 0.7096774193548386\n",
      "in\\muriel18_in.txt 0.7540983606557377\n",
      "in\\muriel26_in.txt 0.7786259541984734\n",
      "in\\muriel27_in.txt 0.7536231884057971\n",
      "in\\muriel30_in.txt 0.6363636363636364\n",
      "lovo\n",
      "{'model': 3, 'param': 0, 'score': 0.7067495840182053}\n"
     ]
    }
   ],
   "source": [
    "# LEAVE ONE VIDEO OUT - LOVO\n",
    "scores=[]\n",
    "for model_id,params in enumerate(models): #1 model = 243 params <- 20 model\n",
    "    for p_idx,videos in enumerate(params): # 21 video = 1 param <- 243 params\n",
    "        score_folds = []\n",
    "        for video_idx in range(len(TIMESERIES)): # 1 prediction = 1 video <- 1 param = 21 video\n",
    "            \n",
    "            train_set = [i for i in range(len(TIMESERIES)) if i != video_idx]\n",
    "            #f1score di 1 fold\n",
    "            _,_,f1,_ = f1scoremargin(gtl,videos,train_set,f1_margin)\n",
    "            score_folds.append(f1)\n",
    "\n",
    "        scores.append({\"model\":model_id, \"param\":p_idx, \"score\":statistics.mean(score_folds)})\n",
    "\n",
    "type_col=[\"model\",\"param\",\"score\"]\n",
    "resexcel=pd.DataFrame()\n",
    "resexcel[\"type\"]=type_col\n",
    "\n",
    "best_models=[]\n",
    "best_score=0\n",
    "for i in scores:\n",
    "    # if better, reset list and append\n",
    "    if i[\"score\"] > best_score:\n",
    "        best_models=[]\n",
    "        best_models.append(i)\n",
    "        best_score = i[\"score\"]\n",
    "    # if equal, append to the list\n",
    "    elif i[\"score\"] == best_score:\n",
    "        best_models.append(i)\n",
    "        \n",
    "best_scores=[]\n",
    "for best_idx,best in enumerate(best_models):\n",
    "    best_score=[]\n",
    "    best_one=0\n",
    "\n",
    "    for video_idx in range(len(TIMESERIES)):\n",
    "        gt = gtl[video_idx]\n",
    "        _,_,f1,_,_,_=f1scoremarginOne(gt,models[best[\"model\"]][best[\"param\"]][video_idx],f1_margin)\n",
    "        best_score.append(f1)\n",
    "        print(f'{TIMESERIES[video_idx]} {f1}')\n",
    "    best_scores.append({\"model\":best[\"model\"], \"param\":best[\"param\"], \"score\":statistics.mean(best_score)})\n",
    "    resexcel[best_idx]=[model_id,p_idx,statistics.mean(best_score)]\n",
    "\n",
    "resexcel.to_excel(\"outputFile/LOVO.xlsx\")\n",
    "print(\"lovo\")\n",
    "for i in best_scores:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lodo\n",
      "{'model': 19, 'param': 242, 'score': 0.7129275024011865}\n",
      "{'model': 19, 'param': 242, 'score': 0.7129275024011865}\n"
     ]
    }
   ],
   "source": [
    "# LEAVE ONE DANCER OUT - LODO\n",
    "CORA=[0,1,2,3,4]\n",
    "MARIANNE=[5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "MURIEL=[17,18,19,20]\n",
    "\n",
    "\n",
    "scores=[]\n",
    "for model_id,params in enumerate(models): #1 model = 243 params <- 20 model\n",
    "    for p_idx,videos in enumerate(params): # 21 video = 1 param <- 243 params\n",
    "        score_folds=[]\n",
    "        \n",
    "        #fold1\n",
    "        TRAINING_1 = CORA+MARIANNE\n",
    "        for ts in TRAINING_1:\n",
    "            #get score each video for ts\n",
    "            #compute score\n",
    "            _,_,f1,_ = f1scoremargin(gtl,videos,TRAINING_1,f1_margin)\n",
    "            score_folds.append(f1)\n",
    "            \"\"\"\n",
    "            gt = gtl[ts]\n",
    "            _,_,f1,_,_,_=f1scoremargin(gt,videos[ts],f1_margin)\n",
    "            score_singlefold_eachvideo.append(f1)\n",
    "        #media per training\n",
    "        fold1_score = statistics.mean(score_singlefold_eachvideo)\n",
    "        \"\"\"\n",
    "            \n",
    "        #fold2\n",
    "        TRAINING_2 = CORA+MURIEL\n",
    "        for ts in TRAINING_2:\n",
    "            _,_,f1,_ = f1scoremargin(gtl,videos,TRAINING_2,f1_margin)\n",
    "            score_folds.append(f1)\n",
    "\n",
    "        #fold3\n",
    "        TRAINING_3 = MARIANNE+MURIEL\n",
    "        for ts in TRAINING_3:\n",
    "            _,_,f1,_ = f1scoremargin(gtl,videos,TRAINING_3,f1_margin)\n",
    "            score_folds.append(f1)\n",
    "\n",
    "        training_mean_score = statistics.mean(score_folds)\n",
    "        scores.append({\"model\":model_id, \"param\":p_idx, \"score\":training_mean_score})\n",
    "\n",
    "type_col=[\"model\",\"param\",\"score\"]\n",
    "resexcel=pd.DataFrame()\n",
    "resexcel[\"type\"]=type_col\n",
    "\n",
    "best_models=[]\n",
    "best_score=0\n",
    "for i in scores:\n",
    "    if i[\"score\"] > best_score:\n",
    "        best_models=[]\n",
    "        best_models.append(i)\n",
    "        best_score=i[\"score\"]\n",
    "    elif i[\"score\"] == best_score:\n",
    "        best_models.append(i)\n",
    "\n",
    "\n",
    "best_scores=[]\n",
    "for best_idx,best in enumerate(best_models):\n",
    "\n",
    "    best_score=[]\n",
    "\n",
    "    _,_,f1,_ = f1scoremargin(gtl,models[best[\"model\"]][best[\"param\"]], CORA, f1_margin)\n",
    "    best_score.append(f1)\n",
    "\n",
    "    _,_,f1,_ = f1scoremargin(gtl,models[best[\"model\"]][best[\"param\"]], MARIANNE, f1_margin)\n",
    "    best_score.append(f1)\n",
    "\n",
    "    _,_,f1,_ = f1scoremargin(gtl,models[best[\"model\"]][best[\"param\"]], MURIEL, f1_margin)\n",
    "    best_score.append(f1)\n",
    "\n",
    "\n",
    "    best_scores.append({\"model\":model_id, \"param\":p_idx, \"score\":statistics.mean(best_score)})\n",
    "    resexcel[best_idx]=[model_id,p_idx,statistics.mean(best_score)]\n",
    "#gg\n",
    "resexcel.to_excel(\"outputFile/LODO.xlsx\")\n",
    "print(\"lodo\")\n",
    "for i in best_scores:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f1scoremargin() missing 1 required positional argument: 'margin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_idx,video \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(videos): \u001b[38;5;66;03m# 1 prediction = 1 video <- 1 param = 21 video\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     gt \u001b[38;5;241m=\u001b[39m gtl[video_idx]\n\u001b[1;32m---> 16\u001b[0m     _,_,f1,_,_,_\u001b[38;5;241m=\u001b[39m\u001b[43mf1scoremargin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf1_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     score_one\u001b[38;5;241m.\u001b[39mappend(f1)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f1 \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: f1scoremargin() missing 1 required positional argument: 'margin'"
     ]
    }
   ],
   "source": [
    "# USE ALL TO BRUTE FORCE AND SCORING\n",
    "f1_margin=100\n",
    "scores=[]\n",
    "for model_id,params in enumerate(models): #1 model = 243 params <- 20 model\n",
    "    for p_idx,videos in enumerate(params): # 21 video = 1 param <- 243 params\n",
    "        score_one = []\n",
    "        best_one=0\n",
    "        vid_b_one=0\n",
    "        worst_one=1\n",
    "        vid_w_one=0\n",
    "        neg_one=0\n",
    "        for video_idx,video in enumerate(videos): # 1 prediction = 1 video <- 1 param = 21 video\n",
    "\n",
    "      \n",
    "            gt = gtl[video_idx]\n",
    "            _,_,f1,_,_,_=f1scoremargin(gt,video,f1_margin)\n",
    "            score_one.append(f1)\n",
    "            if f1 < 0.5:\n",
    "                neg_one+=1\n",
    "            if f1 < worst_one:\n",
    "                worst_one=f1\n",
    "                vid_w_one = video_idx\n",
    "            if f1 > best_one:\n",
    "                best_one=f1\n",
    "                vid_b_one = video_idx\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        scores.append({\"model\":model_id, \"param\":p_idx, \"score\":statistics.mean(score_one), \"neg\":neg_one, \"best\":best_one,\"video_best\":vid_b_one, \"worst\":worst_one, \"video_worst\":vid_w_one})\n",
    "\n",
    "type_col=[\"model\",\"param\",\"score\"]\n",
    "resexcel=pd.DataFrame()\n",
    "resexcel[\"type\"]=type_col\n",
    "best_models=[]\n",
    "best_score=0\n",
    "for i in scores:\n",
    "\n",
    "    # if better, reset list and append\n",
    "    if i[\"score\"] > best_score:\n",
    "        best_models=[]\n",
    "        best_models.append(i)\n",
    "        best_score = i[\"score\"]\n",
    "    # if equal, append to the list\n",
    "    elif i[\"score\"] == best_score:\n",
    "        best_models.append(i)\n",
    "\n",
    "    #resexcel[best_idx]=[model_id,p_idx,statistics.mean(best_score)]\n",
    "\n",
    "resexcel.to_excel(\"outputFile/SCOREALL.xlsx\")\n",
    "print(\"lovo\")\n",
    "for i in best_models:\n",
    "    print(i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
