{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is used to generate prediction given a timeseries or filtered the prediction and save the result on a file.\n",
    "Computing the prediction or filtering it with DTW is time consuming. Instead of computing it each time, just one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loluc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\aeon\\base\\__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from dtaidistance import dtw,similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from claspy.segmentation import BinaryClaSPSegmentation\n",
    "\n",
    "import ruptures as rpt\n",
    "\n",
    "import stumpy\n",
    "from aeon.segmentation import find_dominant_window_sizes\n",
    "\n",
    "from aeon.segmentation import GreedyGaussianSegmenter\n",
    "\n",
    "from aeon.segmentation import InformationGainSegmenter\n",
    "\n",
    "from aeon.anomaly_detection import STRAY\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer,mean_squared_error\n",
    "from ruptures.metrics import precision_recall\n",
    "import matplotlib.pyplot as plt\n",
    "#from aeon.visualisation import plot_series_with_change_points, plot_series_with_profiles\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_color_codes()\n",
    "\n",
    "from claspy.tests.evaluation import f_measure,covering\n",
    "\n",
    "from claspy.window_size import dominant_fourier_frequency, highest_autocorrelation, suss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted features. Use the index of this list to use with iloc[]\n",
    "<ol start=\"0\">\n",
    "  <li>Kinetic Global</li>\n",
    "  <li>Kinetic Chest</li>\n",
    "  <li>Directness Head</li>\n",
    "  <li>Density</li>\n",
    "  <li>left wrist ke</li>\n",
    "  <li>right wrist ke</li>\n",
    "  <li>left ankle ke</li>\n",
    "  <li>right ankle ke</li>\n",
    "  <li>head ke</li>\n",
    "  <li>crouch density</li>\n",
    "  <li>left leg density</li>\n",
    "  <li>right leg density</li>\n",
    "  <li>left hand density</li>\n",
    "  <li>right hand density</li>\n",
    "  <li>head density</li>\n",
    "  <li>arto inferiore</li>\n",
    "  <li>gamba</li>\n",
    "  <li>coscia</li>\n",
    "  <li>coscia dx</li>\n",
    "  <li>coscia sx</li>\n",
    "  <li>gamba sx</li>\n",
    "  <li>gamba dx</li>\n",
    "  <li>braccio sx</li>\n",
    "  <li>braccio dx</li>\n",
    "  <li>avambraccio sx</li>\n",
    "  <li>avambraccio dx</li>\n",
    "  <li>ARIEL speed magnitude</li>\n",
    "  <li>ARIEL speed X component</li>\n",
    "  <li>ARIEL speed Y component</li>\n",
    "  <li>ARIEL speed Z component</li>\n",
    "  <li>ARIEL acceleration magnitude</li>\n",
    "  <li>ARIEL acceleration X component</li>\n",
    "  <li>ARIEL acceleration Y component</li>\n",
    "  <li>ARIEL acceleration Z component</li>\n",
    "  <li>ARIEL jerk magnitude</li>\n",
    "  <li>ARIEL jerk X component</li>\n",
    "  <li>ARIEL jerk Y component</li>\n",
    "  <li>ARIEL jerk Z component</li>\n",
    "  <li>STRN speed magnitude</li>\n",
    "  <li>STRN speed X component</li>\n",
    "  <li>STRN speed Y component</li>\n",
    "  <li>STRN speed Z component</li>\n",
    "  <li>STRN acceleration magnitude</li>\n",
    "  <li>STRN acceleration X component</li>\n",
    "  <li>STRN acceleration Y component</li>\n",
    "  <li>STRN accelerationZ component</li>\n",
    "  <li>STRN jerk magnitude</li>\n",
    "  <li>STRN jerk X component</li>\n",
    "  <li>STRN jerk Y component</li>\n",
    "  <li>STRN jerk Z component</li>\n",
    "  <li>RHEL speed magnitude</li>\n",
    "  <li>RHEL speed X component</li>\n",
    "  <li>RHEL speed Y component</li>\n",
    "  <li>RHEL speed Z component</li>\n",
    "  <li>RHEL acceleration magnitude</li>\n",
    "  <li>RHEL acceleration X component</li>\n",
    "  <li>RHEL acceleration Y component</li>\n",
    "  <li>RHEL acceleration Z component</li>\n",
    "  <li>RHEL jerk magnitude</li>\n",
    "  <li>RHEL jerk X component</li>\n",
    "  <li>RHEL jerk Y component</li>\n",
    "  <li>RHEL jerk Z component</li>\n",
    "  <li>LHEL speed magnitude</li>\n",
    "  <li>LHEL speed X component</li>\n",
    "  <li>LHEL speed Y component</li>\n",
    "  <li>LHEL speed Z component</li>\n",
    "  <li>LHEL acceleration magnitude</li>\n",
    "  <li>LHEL acceleration X component</li>\n",
    "  <li>LHEL acceleration Y component</li>\n",
    "  <li>LHEL acceleration Z component</li>\n",
    "  <li>LHEL jerk magnitude</li>\n",
    "  <li>LHEL jerk X component</li>\n",
    "  <li>LHEL jerk Y component</li>\n",
    "  <li>LHEL jerk Z component</li>\n",
    "  <li>RPLM speed magnitude</li>\n",
    "  <li>RPLM speed X component</li>\n",
    "  <li>RPLM speed Y component</li>\n",
    "  <li>RPLM speed Z component</li>\n",
    "  <li>RPLM acceleration magnitude</li>\n",
    "  <li>RPLM acceleration X component</li>\n",
    "  <li>RPLM acceleration Y component</li>\n",
    "  <li>RPLM acceleration Z component</li>\n",
    "  <li>RPLM jerk magnitude</li>\n",
    "  <li>RPLM jerk X component</li>\n",
    "  <li>RPLM jerk Y component</li>\n",
    "  <li>RPLM jerk Z component</li>\n",
    "  <li>LPLM speed magnitude</li>\n",
    "  <li>LPLM speed X component</li>\n",
    "  <li>LPLM speed Y component</li>\n",
    "  <li>LPLM speed Z component</li>\n",
    "  <li>LPLM acceleration magnitude</li>\n",
    "  <li>LPLM acceleration X component</li>\n",
    "  <li>LPLM acceleration Y component</li>\n",
    "  <li>LPLM acceleration Z component</li>\n",
    "  <li>LPLM jerk magnitude</li>\n",
    "  <li>LPLM jerk X component</li>\n",
    "  <li>LPLM jerk Y component</li>\n",
    "  <li>LPLM jerk Z component</li>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features. To access its name or its value while using iloc\n",
    "features_name=[\n",
    "    \"kinetic_global\",\n",
    "    \"kinetic_chest\",\n",
    "    \"directness_head\",\n",
    "    \"density\",\n",
    "    \"left_wrist_ke\",\n",
    "    \"right_wrist_ke\",\n",
    "    \"left_ankle_ke\",\n",
    "    \"right_ankle_ke\",\n",
    "    \"head_ke\",\n",
    "    \"crouch_density\",\n",
    "    \"left_leg_density\",\n",
    "    \"right_leg_density\",\n",
    "    \"left_hand_density\",\n",
    "    \"right_hand_density\",\n",
    "    \"head_density\",\n",
    "    \"arto_inferiore\",\n",
    "    \"gamba\",\n",
    "    \"coscia\",\n",
    "    \"coscia_dx\",\n",
    "    \"coscia_sx\",\n",
    "    \"gamba_sx\",\n",
    "    \"gamba_dx\",\n",
    "    \"braccio_sx\",\n",
    "    \"braccio_dx\",\n",
    "    \"avambraccio_sx\",\n",
    "    \"avambraccio_dx\",\n",
    "    \"ARIEL_speed_magnitude\",\n",
    "    \"ARIEL_speed_X_component\",\n",
    "    \"ARIEL_speed_Y_component\",\n",
    "    \"ARIEL_speed_Z_component\",\n",
    "    \"ARIEL_acceleration_magnitude\",\n",
    "    \"ARIEL_acceleration_X_component\",\n",
    "    \"ARIEL_acceleration_Y_component\",\n",
    "    \"ARIEL_acceleration_Z_component\",\n",
    "    \"ARIEL_jerk_magnitude\",\n",
    "    \"ARIEL_jerk_X_component\",\n",
    "    \"ARIEL_jerk_Y_component\",\n",
    "    \"ARIEL_jerk_Z_component\",\n",
    "    \"STRN_speed_magnitude\",\n",
    "    \"STRN_speed_X_component\",\n",
    "    \"STRN_speed_Y_component\",\n",
    "    \"STRN_speed_Z_component\",\n",
    "    \"STRN_acceleration_magnitude\",\n",
    "    \"STRN_acceleration_X_component\",\n",
    "    \"STRN_acceleration_Y_component\",\n",
    "    \"STRN_acceleration_Z_component\",\n",
    "    \"STRN_jerk_magnitude\",\n",
    "    \"STRN_jerk_X_component\",\n",
    "    \"STRN_jerk_Y_component\",\n",
    "    \"STRN_jerk_Z_component\",\n",
    "    \"RHEL_speed_magnitude\",\n",
    "    \"RHEL_speed_X_component\",\n",
    "    \"RHEL_speed_Y_component\",\n",
    "    \"RHEL_speed_Z_component\",\n",
    "    \"RHEL_acceleration_magnitude\",\n",
    "    \"RHEL_acceleration_X_component\",\n",
    "    \"RHEL_acceleration_Y_component\",\n",
    "    \"RHEL_acceleration_Z_component\",\n",
    "    \"RHEL_jerk_magnitude\",\n",
    "    \"RHEL_jerk_X_component\",\n",
    "    \"RHEL_jerk_Y_component\",\n",
    "    \"RHEL_jerk_Z_component\",\n",
    "    \"LHEL_speed_magnitude\",\n",
    "    \"LHEL_speed_X_component\",\n",
    "    \"LHEL_speed_Y_component\",\n",
    "    \"LHEL_speed_Z_component\",\n",
    "    \"LHEL_acceleration_magnitude\",\n",
    "    \"LHEL_acceleration_X_component\",\n",
    "    \"LHEL_acceleration_Y_component\",\n",
    "    \"LHEL_acceleration_Z_component\",\n",
    "    \"LHEL_jerk_magnitude\",\n",
    "    \"LHEL_jerk_X_component\",\n",
    "    \"LHEL_jerk_Y_component\",\n",
    "    \"LHEL_jerk_Z_component\",\n",
    "    \"RPLM_speed_magnitude\",\n",
    "    \"RPLM_speed_X_component\",\n",
    "    \"RPLM_speed_Y_component\",\n",
    "    \"RPLM_speed_Z_component\",\n",
    "    \"RPLM_acceleration_magnitude\",\n",
    "    \"RPLM_acceleration_X_component\",\n",
    "    \"RPLM_acceleration_Y_component\",\n",
    "    \"RPLM_acceleration_Z_component\",\n",
    "    \"RPLM_jerk_magnitude\",\n",
    "    \"RPLM_jerk_X_component\",\n",
    "    \"RPLM_jerk_Y_component\",\n",
    "    \"RPLM_jerk_Z_component\",\n",
    "    \"LPLM_speed_magnitude\",\n",
    "    \"LPLM_speed_X_component\",\n",
    "    \"LPLM_speed_Y_component\",\n",
    "    \"LPLM_speed_Z_component\",\n",
    "    \"LPLM_acceleration_magnitude\",\n",
    "    \"LPLM_acceleration_X_component\",\n",
    "    \"LPLM_acceleration_Y_component\",\n",
    "    \"LPLM_acceleration_Z_component\",\n",
    "    \"LPLM_jerk_magnitude\",\n",
    "    \"LPLM_jerk_X_component\",\n",
    "    \"LPLM_jerk_Y_component\",\n",
    "    \"LPLM_jerk_Z_component\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1scoremargin(ground_truth, predictions, tolerance):\n",
    "    \"\"\"\n",
    "    Calcola l'F1 score con una finestra di tolleranza sui change points.\n",
    "    \n",
    "    :param ground_truth: Lista o array di change points reali\n",
    "    :param predictions: Lista o array di change points predetti\n",
    "    :param tolerance: La tolleranza temporale (numero di unità temporali)\n",
    "    :return: precision, recall, f1-score\n",
    "    \"\"\"\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Vettori per tracciare quali punti sono stati già associati\n",
    "    matched_ground_truth = np.zeros(len(ground_truth), dtype=bool)\n",
    "    matched_predictions = np.zeros(len(predictions), dtype=bool)\n",
    "\n",
    "    mgt={key: False for key in ground_truth}\n",
    "    mcp={key: False for key in predictions}\n",
    "    #print(f'gt:{len(ground_truth)} - cp:{len(predictions)}')\n",
    "    # True Positives (TP)\n",
    "    tp = 0\n",
    "    for i, gt_point in enumerate(ground_truth):\n",
    "        for j, pred_point in enumerate(predictions):\n",
    "            \n",
    "            if not matched_predictions[j] and abs(gt_point - pred_point) <= tolerance:\n",
    "                tp += 1\n",
    "                matched_ground_truth[i] = True\n",
    "                matched_predictions[j] = True\n",
    "\n",
    "                mgt[gt_point] = True\n",
    "                mcp[pred_point] = True\n",
    "                break\n",
    "            \n",
    "    \n",
    "    # False Positives (FP) - predizioni non corrispondenti a nessun ground truth entro la tolleranza\n",
    "    fp = np.sum(~matched_predictions)\n",
    "    \n",
    "    # False Negatives (FN) - punti del ground truth non corrispondenti a nessuna predizione entro la tolleranza\n",
    "    fn = np.sum(~matched_ground_truth)\n",
    "    #print(f'tp:{tp} - fp:{fp} - fn:{fn}')\n",
    "    #print(mgt)\n",
    "    #print(mcp)\n",
    "    # Calcolo di precision, recall e F1-score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    #print(f'gt:{len(ground_truth)} cp:{len(predictions)} tp:{tp} fp:{fp} fn:{fn}')\n",
    "    return precision, recall, f1, {\"tp\":tp, \"fp\":fp, \"fn\":fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadAndPreProcess(inputDataRaw):\n",
    "    # lettura\n",
    "    df=pd.read_csv(inputDataRaw,sep=' ', header=None).interpolate()\n",
    "    \n",
    "    df=df.drop(0, axis=1)\n",
    "    df=df.drop_duplicates()\n",
    "    df = df.iloc[:, ::-1]\n",
    " \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questa funzione ritorna un dataframe del groundtruth che viene usato specificatamente per visualizzare il gt\n",
    "# è soggetto a un preprocessing dei dati siccome l'ultimo groundtruth è dove termina il ts del gt\n",
    "# di conseguenza per farlo corrispondere, bisogna stretcharlo\n",
    "# ma ricordo di aver rifatti i dati nuovi per generare un groundtruth a fine ts, da controllare cosi che non serve stretcharlo?\n",
    "def LoadingGroundTruth(df,gtraw):\n",
    "    gt=pd.read_csv(gtraw,sep=' ', header=None)\n",
    "    gt=gt.iloc[:,0].values\n",
    "    #stretching dei dati se necessario per farlo corrispondere alla ts dei dati\n",
    "    stretch_gt = np.array([])\n",
    "    for idx,i in enumerate(gt):\n",
    "        relpos = len(df)*i/gt[-1]\n",
    "        stretch_gt = np.append(stretch_gt,relpos)\n",
    "\n",
    "    # eliminiamo l'ultimo elemento che è stato annotato solo per delimitare la lunghezza della gt simile alla ts\n",
    "    \n",
    "    return stretch_gt[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetClasp2(df,gt,known,feature, **kwargs):\n",
    "    \n",
    "    result=np.array([])\n",
    "    eachresult = []\n",
    "    eachclasp=[]\n",
    "    for i in feature:\n",
    "    \n",
    "        ts=df.iloc[:,i]\n",
    "        \n",
    "        #print(ts.head())\n",
    "        if known == 1:\n",
    "            #print(\"knwon!\")\n",
    "            clasp = BinaryClaSPSegmentation(n_segments=len(gt), validation=None)\n",
    "        else:\n",
    "            #print(\"unknown!\")\n",
    "            clasp = BinaryClaSPSegmentation(**kwargs)\n",
    "            \n",
    "        found_cps = clasp.fit_predict(ts.values)    \n",
    "\n",
    "        # c'è un bug con binseg dove un cp è oltre la lunghezza del ts\n",
    "        # faccio un loop e se eccede cambio il valore con la len(tf)-1\n",
    "        # WTF IS THIS\n",
    "        \"\"\"\n",
    "        for i in range(0,len(found_cps)):\n",
    "            if found_cps[i] >= len(ts):\n",
    "                found_cps[i] = len(ts)-1\n",
    "        \"\"\"\n",
    "        # per ogni array di cp di ogni singola feature\n",
    "        # li unisco in un unico array. in pratica faccio un OR di tutti i cp\n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        #potenziale bug.\n",
    "        #se faccio unique() mi toglie il numero di cp in un punto e quando faccio majority voting mi si toglie\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "        eachclasp.append(clasp)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    return result, eachresult, eachclasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizzo CLASP \n",
    "# prende come parametro un dataframe e restituisce il clasp score\n",
    "# gt e known vengono usati per usare il numero vero di cp se uguale a 1 sennò si cerca di predirlo se il modello lo permette\n",
    "def GetClasp3(df,gt,known,feature, **kwargs):\n",
    "    \n",
    "    result=np.array([])\n",
    "    eachresult = []\n",
    "    eachclasp=[]\n",
    "    for i in [feature]:\n",
    "    \n",
    "        ts=df.iloc[:,i]\n",
    "        \n",
    "        #print(ts.head())\n",
    "        if known == 1:\n",
    "            #print(\"knwon!\")\n",
    "            clasp = BinaryClaSPSegmentation(n_segments=len(gt), validation=None)\n",
    "        else:\n",
    "            #print(\"unknown!\")\n",
    "            clasp = BinaryClaSPSegmentation(**kwargs)\n",
    "            \n",
    "        found_cps = clasp.fit_predict(ts.values)    \n",
    "\n",
    "        # c'è un bug con binseg dove un cp è oltre la lunghezza del ts\n",
    "        # faccio un loop e se eccede cambio il valore con la len(tf)-1\n",
    "        # WTF IS THIS\n",
    "        \"\"\"\n",
    "        for i in range(0,len(found_cps)):\n",
    "            if found_cps[i] >= len(ts):\n",
    "                found_cps[i] = len(ts)-1\n",
    "        \"\"\"\n",
    "\n",
    "        # per ogni array di cp di ogni singola feature\n",
    "        # li unisco in un unico array. in pratica faccio un OR di tutti i cp\n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "        eachclasp.append(clasp)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    return result, eachresult, eachclasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizzo CLASP \n",
    "# prende come parametro un dataframe e restituisce il clasp score\n",
    "# gt e known vengono usati per usare il numero vero di cp se uguale a 1 sennò si cerca di predirlo se il modello lo permette\n",
    "def GetClasp(df,gt,known, **kwargs):\n",
    "    \n",
    "    result=np.array([])\n",
    "    eachresult = []\n",
    "    eachclasp=[]\n",
    "    for i in range(0,len(features_name)):\n",
    "    \n",
    "        ts=df.iloc[:,i]\n",
    "        \n",
    "        #print(ts.head())\n",
    "        if known == 1:\n",
    "            #print(\"knwon!\")\n",
    "            clasp = BinaryClaSPSegmentation(n_segments=len(gt), validation=None)\n",
    "        else:\n",
    "            #print(\"unknown!\")\n",
    "            clasp = BinaryClaSPSegmentation(**kwargs)\n",
    "            \n",
    "        found_cps = clasp.fit_predict(ts.values)    \n",
    "\n",
    "        # c'è un bug con binseg dove un cp è oltre la lunghezza del ts\n",
    "        # faccio un loop e se eccede cambio il valore con la len(tf)-1\n",
    "        # WTF IS THIS\n",
    "        \"\"\"\n",
    "        for i in range(0,len(found_cps)):\n",
    "            if found_cps[i] >= len(ts):\n",
    "                found_cps[i] = len(ts)-1\n",
    "        \"\"\"\n",
    "        # per ogni array di cp di ogni singola feature\n",
    "        # li unisco in un unico array. in pratica faccio un OR di tutti i cp\n",
    "        result = np.sort(np.append(result,found_cps).flatten())\n",
    "        result = np.unique(result)\n",
    "        eachresult.append(found_cps)\n",
    "        eachclasp.append(clasp)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    return result, eachresult, eachclasp\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPremoverDTW(featureTS,res):\n",
    "     # controlla quale segmento piu lungo o corto\n",
    "    # si fa una finestra grande quanto corto e slitta su lungo\n",
    "    # si fa un dtw per ogni finestra e si prende lo score piu basso\n",
    "    # se entro un certo limite si unisce, senno si continua\n",
    "    to_color=[]\n",
    "    filtered=[res[0]]\n",
    "    lo = 0\n",
    "    hi = 1\n",
    "    while hi < len(res)-1:\n",
    "       # print(f'hi:{hi} max:{len(res)}')\n",
    "        a = featureTS[int(res[lo]):int(res[hi])]\n",
    "        b = featureTS[int(res[hi]):int(res[hi+1])]\n",
    "        # controllo quale segmento piu lungo o corto\n",
    "        # halflo = segmento piu corto è una timeseries\n",
    "        # halfhi = segmento piu lungo è una timeseries\n",
    "        if len(a) < len(b):\n",
    "            halflo=featureTS[int(res[lo]):int(res[hi])]\n",
    "            halfhi=featureTS[int(res[hi]):int(res[hi+1])]\n",
    "        else:\n",
    "            halfhi=featureTS[int(res[lo]):int(res[hi])]\n",
    "            halflo=featureTS[int(res[hi]):int(res[hi+1])]\n",
    "\n",
    "        # faccio sliding window e trovo la distance piu piccola\n",
    "        st = 0\n",
    "        en = len(halflo) # possibile bug\n",
    "        smallestdist=-1\n",
    "       # print(f'en:{en} whilelim:{len(halfhi)}')\n",
    "        while en < len(halfhi):\n",
    "            \n",
    "            slidewin = halfhi[st:en]\n",
    "            distance, paths = dtw.warping_paths(halflo, slidewin, use_c=False)\n",
    "            best_path = dtw.best_path(paths)\n",
    "            similarity_score = distance / len(best_path)\n",
    "            if smallestdist == -1:\n",
    "                smallestdist = distance\n",
    "            elif smallestdist > distance:\n",
    "                smallestdist = distance\n",
    "\n",
    "            st+=50\n",
    "            en+=50\n",
    "            #print(f'en:{en} lim:{len(halfhi)} dist:{distance}')\n",
    "       # print(f'smallest:{smallestdist}')\n",
    "        if smallestdist <= 100:\n",
    "            to_color.append(hi)\n",
    "            hi+=1\n",
    "        else:\n",
    "            filtered.append(res[hi])\n",
    "            lo=hi\n",
    "            hi=lo+1\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries=[\n",
    "    \"in\\cora1_in.txt\",\n",
    "      \"in\\cora4_05_in.txt\",\n",
    "      \"in\\cora4_08_in.txt\",\n",
    "      \"in\\cora5_in.txt\",\n",
    "      \"in\\cora14_in.txt\",\n",
    "      \"in\\marianne7_in.txt\",\n",
    "      \"in\\marianne8_in.txt\",\n",
    "      \"in\\marianne10_in.txt\",\n",
    "      \"in\\marianne18_in.txt\",\n",
    "      \"in\\marianne19_in.txt\",\n",
    "      \"in\\marianne24_in.txt\",\n",
    "      \"in\\marianne26_in.txt\",\n",
    "      \"in\\marianne41_in.txt\",\n",
    "      \"in\\marianne42_in.txt\",\n",
    "      \"in\\marianne43_in.txt\",\n",
    "      \"in\\marianne47_in.txt\",\n",
    "      \"in\\marianne48_in.txt\",\n",
    "      \"in\\muriel18_in.txt\",\n",
    "      \"in\\muriel26_in.txt\",\n",
    "      \"in\\muriel27_in.txt\",\n",
    "      \"in\\muriel30_in.txt\"\n",
    "\n",
    "      ]\n",
    "groundtruth=[\n",
    "         \"gt\\cora_gt_2019-08-08_t001_video01.txt\",\n",
    "         \"gt\\cora_gt_2019-05-22_t004_video01.txt\",\n",
    "         \"gt\\cora_gt_2019-08-08_t004_video01.txt\",\n",
    "         \"gt\\cora5_gt.txt\",\n",
    "         \"gt\\cora_gt_2019-08-08_t014_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t007_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t008_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t010_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t018_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t019_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t024_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t026_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t041_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t042_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t043_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t047_video01.txt\",\n",
    "         \"gt\\marianne_gt_2016-03-22_t048_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-21_t018_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-21_t026_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-21_t027_video01.txt\",\n",
    "         \"gt\\muriel_gt_2016-03-23_t030_video01.txt\"\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "with open(\"../dfl.pkl\", \"rb\") as f:\n",
    "    dfl = pickle.load(f)\n",
    "\n",
    "with open(\"../gtl.pkl\", \"rb\") as f:\n",
    "    gtl = pickle.load(f)\n",
    "\n",
    "#suss\n",
    "with open(\"../cpsl.pkl\", \"rb\") as f:\n",
    "    cpsl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>FOR COMPUTING THE PREDICTION</b></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute all cp for all timeseries, so later we can use it freely\n",
    "dfl_st_z=[]\n",
    "gtl_st_z=[]\n",
    "cpsl_st_z=[]\n",
    "for i in range(len(timeseries)):\n",
    "    print(f'PRIMO LOOP:{i}/{len(timeseries)}')\n",
    "    df_raw=ReadAndPreProcess(timeseries[i])\n",
    "    scaler = StandardScaler()\n",
    "    df_scaler = scaler.fit_transform(df_raw)\n",
    "    df = pd.DataFrame(df_scaler)\n",
    "\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp_all,cps,clasp=GetClasp2(df,gt,0,list(range(len(features_name))), window_size=50,distance=\"znormed_euclidean_distance\",n_jobs=6)\n",
    "    dfl_st_z.append(df)\n",
    "    gtl_st_z.append(gt)\n",
    "    cpsl_st_z.append(cps)\n",
    "\n",
    "with open(\"dfl_st_z.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dfl_st_z, f)\n",
    "\n",
    "with open(\"gtl_st_z.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gtl_st_z, f)\n",
    "\n",
    "with open(\"cpsl_st_z.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cpsl_st_z, f)\n",
    "\n",
    "print(\"COMPLETATO PRIMA PARTE\")\n",
    "\n",
    "# compute all cp for all timeseries, so later we can use it freely\n",
    "dfl_no_z=[]\n",
    "gtl_no_z=[]\n",
    "cpsl_no_z=[]\n",
    "for i in range(len(timeseries)):\n",
    "    print(f'SECONDO LOOP:{i}/{len(timeseries)}')\n",
    "    df_raw=ReadAndPreProcess(timeseries[i])\n",
    "    normalizer = MinMaxScaler()\n",
    "    df_normalizer = normalizer.fit_transform(df_raw)\n",
    "    df=pd.DataFrame(df_normalizer)\n",
    "    gt=LoadingGroundTruth(df,groundtruth[i])\n",
    "    cp_all,cps,clasp=GetClasp2(df,gt,0,list(range(len(features_name))), window_size=\"suss\",distance=\"znormed_euclidean_distance\",n_jobs=6)\n",
    "    dfl_no_z.append(df)\n",
    "    gtl_no_z.append(gt)\n",
    "    cpsl_no_z.append(cps)\n",
    "\n",
    "with open(\"dfl_no_z.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dfl_no_z, f)\n",
    "\n",
    "with open(\"gtl_no_z.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gtl_no_z, f)\n",
    "\n",
    "with open(\"cpsl_no_z.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cpsl_no_z, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>FOR FILTERING WITH DTW</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing: 0/21\n",
      "feature:0\n",
      "feature:1\n",
      "feature:2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,cp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cps):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     new_cp\u001b[38;5;241m=\u001b[39m\u001b[43mFPremoverDTW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     new_cps\u001b[38;5;241m.\u001b[39mappend(new_cp)\n\u001b[0;32m     12\u001b[0m new_cpsl\u001b[38;5;241m.\u001b[39mappend(new_cps)\n",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m, in \u001b[0;36mFPremoverDTW\u001b[1;34m(featureTS, res)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m en \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(halfhi):\n\u001b[0;32m     31\u001b[0m     slidewin \u001b[38;5;241m=\u001b[39m halfhi[st:en]\n\u001b[1;32m---> 32\u001b[0m     distance, paths \u001b[38;5;241m=\u001b[39m \u001b[43mdtw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarping_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhalflo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslidewin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_c\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     best_path \u001b[38;5;241m=\u001b[39m dtw\u001b[38;5;241m.\u001b[39mbest_path(paths)\n\u001b[0;32m     34\u001b[0m     similarity_score \u001b[38;5;241m=\u001b[39m distance \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(best_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\dtaidistance\\dtw.py:479\u001b[0m, in \u001b[0;36mwarping_paths\u001b[1;34m(s1, s2, psi_neg, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m dtw[i1, j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmin\u001b[39m(dtw[i0, j],\n\u001b[0;32m    476\u001b[0m                          dtw[i0, j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m s\u001b[38;5;241m.\u001b[39madj_penalty,\n\u001b[0;32m    477\u001b[0m                          dtw[i1, j] \u001b[38;5;241m+\u001b[39m s\u001b[38;5;241m.\u001b[39madj_penalty)\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# dtw[i + 1, j + 1 - skip] = d + min(dtw[i + 1, j + 1 - skip], dtw[i + 1, j - skip])\u001b[39;00m\n\u001b[1;32m--> 479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtw[i1, j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m s\u001b[38;5;241m.\u001b[39madj_max_dist:\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m smaller_found:\n\u001b[0;32m    481\u001b[0m         sc \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_cpsl=[]\n",
    "for i in range(len(timeseries)):\n",
    "    print(f\"computing: {i}/{len(timeseries)}\")\n",
    "    df=dfl[i]\n",
    "    gt=gtl[i]\n",
    "    cps=np.array(cpsl[i],dtype=\"object\")\n",
    "    new_cps=[]\n",
    "    for idx,cp in enumerate(cps):\n",
    "        print(f'feature:{idx}')\n",
    "        new_cp=FPremoverDTW(df.iloc[:,idx].values,cp)\n",
    "        new_cps.append(new_cp)\n",
    "    new_cpsl.append(new_cps)\n",
    "\n",
    "with open(\"../cpsl_suss_DTW.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_cpsl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
